* Disclaimer

Первое знакомство с Kubernetes должно начинаться, конечно, не с того,
что такое Pod или какая-Либо другая абстракция, а что вообще такое из
себя представляет Kubernetes. Кому и зачем он вообще нужен и нужен ли
в принципе. Возможно необходима краткая историческая справка. Но я
оставлю я этот вопрос или на потом или вообще не коснусь его ибо все
из присутствующих пришли сюда добровольно и, вероятнее всего, успели
погуглить что это и приследуют свои собственные цели.

* Pod и что он из себя представляет

В первую очередь, когда мы говорим Pod, имея некоторый опыт в Docker
мы подразумеваем контейнер, в котором запущено одно или несколько
приложений, но есть нюансы. О них я скажу далее. В первую очерень нам
надо запомнить, что Pod в Kubernetes это минимальная контейнерная
величина в Kubernetes, базовый кирпичик, который создается и умирает
исключительно целиком, этот обьект предоставляется нам API Kubernetes
и оперировать меньшими частями мы не в силах.

** Отличия от Docker-контейнера

Почему бы нам не назвать Pod используя терминологию из Docker:
"контейнер"? Мы не можем сделать это по нескольким причинам.

Для людей мало знакомым с Docker постараюсь дать вводную кратко -
Docker-контейнер это запущенный экземляр образа, мы можем переносить
Docker-образ с нашим приложением и данными между машинами, загружать
его в удаленное хранилище (реестр образов Docker) для долгосрочного
хранения, когда мы запускаем образ мы получаем контейнер, в котором
можем выполнять любые желаемые нами команды.

Итак, отличия Pod'а-Kubernetes от Docker-контейнера: Pod это группа
контейнеров (группа из одного контейнера как частный случай)

** Что дает нам Pod

Хорошей практикой в Docker является запуск одного приложения на
контейнер. При этом Docker не дает нам никакой возможности описать
зависимость между контейнерами (порядок запуска, перезагрузка группы
в случае остановки одного из контейнеров, группировка при выполнении
запросов (хотя это можно сделать))

Когда наше приложение разделено между несколькими контейнерами, чаще
всего требуется доступ из одного контейнера в другой. Pod позволяет
описать это взаимодейстивие. Это может быть общая директория или
TCP-соединение. И тут Kubernetes нас сразу ставит перед данностью, что
контейнеры, запущенные в рамках одного пода имеют общий localhost,
т.е. запустить два Nginx в рамках пода на одном порту у нас не
получится.

А при смерти одного из контейнеров у нас умирает pod целиком.

** Как создаются pod'ы?

Pod'ы как и любые другие ресурсы Kubernetes создаются запросом в API.
От нас остается только сделать этот запрос. Мы можем сделать это двумя
путями: вызовом kubectl c указанием необходимых параметров,
если что-то надо сделать единоразово и быстро (но лучше бы не спешить)
или передачей файла-манифеста с описанием ресурса в формате yaml или
json

** Создание простейшего pod через kubectl

#+BEGIN_EXAMPLE
$ kubectl run --generator=run-pod/v1 --image=alpine foo sleep 1h
#+END_EXAMPLE

Взглянем на запущенный под

#+BEGIN_EXAMPLE
$ kubectl get pods
NAME   READY   STATUS    RESTARTS   AGE
foo    1/1     Running   0          7s
#+END_EXAMPLE

Статус нам намекат что все хорошо, и это не может не радовать

** Создание простейшего pod из файла-манифеста

Тут уже немного сложнее, для начала нам потребуется создать файл
00-bar.yaml (название может быть любым)

И наполнить его следующим содержимым:

#+BEGIN_EXAMPLE
apiVersion: v1
kind: Pod
metadata:
  name: bar
spec:
  containers:
  - name: bar
    image: alpine
    command: ["sleep"]
    args: ["1h"]
#+END_EXAMPLE

Проверим список подов теперь:

#+BEGIN_EXAMPLE
$ kubectl get pods
NAME   READY   STATUS    RESTARTS   AGE
bar    1/1     Running   0          6s
foo    1/1     Running   0          9m53s
#+END_EXAMPLE

Отлично! Теперь у нас два пода, но что с ними можно делать дальше?

** Эксплуатация

Когда наше приложение запущено в Kubernetes в рамках одного Pod нам
необходимо знать как обслуживать и отлаживать наше новоиспеченное
приложение, с этим нам может помочь несколько полезных команд

Получение логов из контейнера:
#+BEGIN_EXAMPLE
$ kubectl logs bar
#+END_EXAMPLE

Запуск приложения в контейнере в интерактивном режиме с захватом
STDIN:
#+BEGIN_EXAMPLE
$ kubectl exec -it bar sh
/ #
#+END_EXAMPLE

Получение полной информации о запущенном pod:
#+BEGIN_EXAMPLE
$ kubectl describe pod bar
Name:         bar
Namespace:    default
Priority:     0
Node:         247152.local/188.225.47.246
Start Time:   Wed, 18 Sep 2019 00:34:11 +0300
Labels:       <none>
Annotations:  cni.projectcalico.org/podIP: 192.168.234.96/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"bar","namespace":"default"},"spec":{"containers":[{"args":["1h"],"com...
Status:       Running
IP:           192.168.234.96
Containers:
  bar:
    Container ID:  docker://4ffd83ea4d94c98cb34cde38b138d4861950d8ee0a52623c88de092466548a50
    Image:         alpine
    Image ID:      docker-pullable://alpine@sha256:72c42ed48c3a2db31b7dafe17d275b634664a708d901ec9fd57b1529280f01fb
    Port:          <none>
    Host Port:     <none>
    Command:
      sleep
    Args:
      1h
    State:          Running
      Started:      Wed, 18 Sep 2019 00:34:15 +0300
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mmz7p (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-mmz7p:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-mmz7p
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From                   Message
  ----    ------     ----   ----                   -------
  Normal  Scheduled  10m    default-scheduler      Successfully assigned default/bar to 247152.local
  Normal  Pulling    9m58s  kubelet, 247152.local  Pulling image "alpine"
  Normal  Pulled     9m56s  kubelet, 247152.local  Successfully pulled image "alpine"
  Normal  Created    9m56s  kubelet, 247152.local  Created container bar
  Normal  Started    9m55s  kubelet, 247152.local  Started container bar
#+END_EXAMPLE

Остановка запущенного pod:
#+BEGIN_EXAMPLE
$ kubectl delete pod bar
pod "bar" deleted
#+END_EXAMPLE

** Проблемы подобного Podхода

Обновить подобный контейнер без простоя - невозможно, между
перезапусками неизбежной пройдет время, которое заменят пользователи
и явно не одобрят. Но у Kubernetes на это есть ответ в виде абстракций
более высокого уровня.

Pod хоть и состоит из множества контейнеров, но при этом они могут
находиться одновременно только на одной машине кластера, поэтому
горизонтальное масштабирование в данном случае нам так же не доступно

* ReplicaSet

ReplicaSet - это группа из множества Pod'ов которые мы можем
масштабировать вверх или вниз в любой момент времени всего одной
командой. Контроллер, который отвечает за ReplicaSet автоматически
сгенерирует уникальные имена подов а Kubernetes распределит запущенные
реплики по машинам оптимальным образом без нашего участия.

** Создание из файла-манифеста

Создадим файл 01-bar.yaml следующего содержания:
#+BEGIN_EXAMPLE
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: bar
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bar
  template:
    metadata:
      labels:
        app: bar
    spec:
      containers:
      - name: bar
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "trap 'exit 0' 15;while true; do exec sleep 100 & wait $!; done"]
#+END_EXAMPLE

Применим манифест:
#+BEGIN_EXAMPLE
$ kubectl apply -f 01-bar.yaml
#+END_EXAMPLE

И проверим теперь список подов:

#+BEGIN_EXAMPLE
$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
bar         1/1     Running   1          39m
bar-78mnv   1/1     Running   0          12s
bar-f675p   1/1     Running   0          12s
bar-w688v   1/1     Running   0          12s
foo         1/1     Running   1          74m
#+END_EXAMPLE

Как видим, теперь наше приложение имеет 3 реплики, что вероятнее всего
обрадует наших воображаемых клиентов заментно возросшим uptime

** Эксплуатация

В любой момент времени мы можем отмасштабировать наш ReplicaSet:

#+BEGIN_EXAMPLE
$ kubectl scale replicaset bar --replicas=5
#+END_EXAMPLE

И, разумеется, удалить
#+BEGIN_EXAMPLE
$ kubectl delete replicaset bar
#+END_EXAMPLE

** Недостатки

Хоть теперь наше приложение может масштабироваться и останется
доступным при потере какой-либо ноды кластера мы все равно на данный
момент не можем провести обновление, которое бы осталось незаметным
для наших воображаемых клиентов

* Deployment

Этот ресурс уже лишен всех недостатков перечисленных ранее, при
изменении конфигурации контейнеров данного ресурса по-умолчанию
начнется плавная (rollout) замена каждого из реплики на новую версию
пока каждая реплика не станет актуальной версией

** Создание из файла-манифеста

Создадим файл 02-bar.yaml следующего содержания:
#+BEGIN_EXAMPLE
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bar
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bar
  template:
    metadata:
      labels:
        app: bar
    spec:
      containers:
      - name: bar
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "trap 'exit 0' 15;while true; do exec sleep 100 & wait $!; done"]
#+END_EXAMPLE

Применим манифест:
#+BEGIN_EXAMPLE
$ kubectl apply -f 02-bar.yaml
#+END_EXAMPLE

И проверим теперь список подов:

#+BEGIN_EXAMPLE
$ kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
bar-65dbfbcf46-fjmwj   1/1     Running   0          5m18s
bar-65dbfbcf46-mcv5j   1/1     Running   0          5m18s
bar-65dbfbcf46-wb8s6   1/1     Running   0          5m18s
#+END_EXAMPLE

Попробуем изменить образ с alpine на debian:stable-slim

#+BEGIN_EXAMPLE
$ sed -i 's/alpine/debian:stable-slim/g' 02-bar.yam
$ kubectl apply -f 02-bar.yaml
deployment.apps/bar configured
$ kubectl get pods
NAME                   READY   STATUS              RESTARTS   AGE
bar-65dbfbcf46-fjmwj   1/1     Running             0          7m13s
bar-65dbfbcf46-mcv5j   1/1     Running             0          7m13s
bar-65dbfbcf46-wb8s6   1/1     Running             0          7m13s
bar-68c6b49ffc-mvqfk   0/1     ContainerCreating   0          4s

# Некоторое время спустя

$ kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
bar-68c6b49ffc-5lm9l   1/1     Running   0          54s
bar-68c6b49ffc-mvqfk   1/1     Running   0          65s
bar-68c6b49ffc-nkw55   1/1     Running   0          61s
#+END_EXAMPLE

** Эксплуатация

В любой момент времени мы можем отмасштабировать наш Deployment:
#+BEGIN_EXAMPLE
$ kubectl scale deployment bar --replicas=5
#+END_EXAMPLE

Удаление Deployment:
#+BEGIN_EXAMPLE
$ kubectl delete deployment bar
#+END_EXAMPLE

* PersistentVolumeClaim

У всего перечисленного выше есть не то чтобы недостаток, скорее
ограничение: при перезапуске контейнера мы теряем все данные внутри.
Это не проблема для stateless-сервисов, но когда речь заходит за базу
данных или файлы загружаемые пользователем (stateful-сервисы) нам
необходимо где-то хранить эти данные. В Kubernetes это самая сложная
тема, к счастью с ней столкнутся только те, кто решил развернуть свой
кластер на BareMetal в остальных случаях эту проблему берет на себя
облачный провайдер

** Создание из файла-манифеста

Создадим файл 03-baz-pvc.yaml следующего содержания:
#+BEGIN_EXAMPLE
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: baz
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
#+END_EXAMPLE

Coздадим и проверим статус PersistentVolumeClaim

#+BEGIN_EXAMPLE
$ kubectl apply -f 03-baz-pvc.yaml
persistentvolumeclaim/baz created
$ kubectl get pvc
NAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
baz    Bound    pv0001   2Gi        RWO                           7s
#+END_EXAMPLE

** Использование в Pod

Создадим файл 03-baz.yaml следующего содержания:

#+BEGIN_EXAMPLE
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
#+END_EXAMPLE

Попробуем применить этот манифест и проверить, что бы теперь
действительно можем хранить состояние между перезапусками контейнера

#+BEGIN_EXAMPLE
$ kubectl apply -f 03-baz.yaml
pod/baz created
$ kubectl exec baz ls -- -la /pvc
total 8
drwxr-xr-x    2 root     root          4096 Sep 18 04:18 .
drwxr-xr-x    1 root     root          4096 Sep 18 04:22 ..
$ kubectl exec baz touch -- /pvc/test
$ kubectl exec baz ls -- -la /pvc
total 8
drwxr-xr-x    2 root     root          4096 Sep 18 04:24 .
drwxr-xr-x    1 root     root          4096 Sep 18 04:23 ..
-rw-r--r--    1 root     root             0 Sep 18 04:24 test
$ kubectl exec baz kill -- 1
$ kubectl get pods
NAME   READY   STATUS      RESTARTS   AGE
baz    0/1     Completed   0          2m41s

# Чуть позже

$ kubectl get pods
NAME   READY   STATUS    RESTARTS   AGE
baz    1/1     Running   1          2m46s
$ kubectl exec baz ls -- -la /pvc
total 8
drwxr-xr-x    2 root     root          4096 Sep 18 04:24 .
drwxr-xr-x    1 root     root          4096 Sep 18 04:24 ..
-rw-r--r--    1 root     root             0 Sep 18 04:24 test
#+END_EXAMPLE

** Эксплуатация

Получение списка PersistentVolumeClaim:
#+BEGIN_EXAMPLE
$ kubectl get pvc

NAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
baz    Bound    pv0001   2Gi        RWO                           4h59m
#+END_EXAMPLE

** Недостатки

К сожалению данный подход имеет все недостатки одиночного подхода в
виде проблем с масштабированием, отказоустойчивостью и наличием
простоя при обновлении контейнера

* Service

Прежде чем пы познакомимся с последней на сегодня абстракцией, которая
управляет непосредственно pod'ами нам необходимо познакомиться с
другой абстракцией - Service.

Service как ресурс в рамках Kubernetes отвечает за единую точку входа
к однотипным контейнерам. Ранее мы мы создавали реплики приложения,
но всегда обращились к какой-либо конкретной реплике по ее уникальному
имени, что крайне неудобно для внешнего клиента, которого, на самом
деле, не интересуют детали нашей инфраструктуры, он хочет обратьться
по единому адресу и ожидает рабочий сервис по этому адресу.

** Создание из файла-манифеста

Для создания сервиса в кластере, который будет перенаправлять запросы
в контейнеры с metadata.labels.app = qux. Опробовать на ресурс
Service я предлагаю чуть позже, а пока просто знать и помнить что он
существует, очень скоро он нам понадобится

#+END_EXAMPLE
apiVersion: v1
kind: Service
metadata:
  name: qux
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: qux
#+END_EXAMPLE

* StatefulSet

StatefulSet - это почти как Deployment, только с состоянием

** Создание из файла-манифеста

Создадим файл 04-bar-service.yaml сдедующего содержания:

#+BEGIN_EXAMPLE
apiVersion: v1
kind: Service
metadata:
  name: bar
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: bar
#+END_EXAMPLE

И файл 04-bar.yaml сдедующего содержания:

#+BEGIN_EXAMPLE
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: bar
spec:
  selector:
    matchLabels:
      app: bar
  replicas: 2
  serviceName: bar
  template:
    metadata:
      labels:
        app: bar
    spec:
      containers:
      - name: nginx
        image: nginxdemos/hello:plain-text
        volumeMounts:
        - name: bar
          mountPath: /pvc
  volumeClaimTemplates:
  - metadata:
      name: bar
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 2Gi
#+END_EXAMPLE

Посмотрим список pod'ов:

#+BEGIN_EXAMPLE
root@247152:~# kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
bar-0   1/1     Running   0          6m12s
bar-1   1/1     Running   0          4m30s
#+END_EXAMPLE

Теперь мы можем проверить как работает Service с нашим ReplicaSet:

#+BEGIN_EXAMPLE
$ kubectl run --generator=run-pod/v1 --image=alpine test sleep 1h
pod/test created
$ kubectl exec -it test sh
/ # apk add --no-cache curl
fetch http://dl-cdn.alpinelinux.org/alpine/v3.10/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.10/community/x86_64/APKINDEX.tar.gz
(1/4) Installing ca-certificates (20190108-r0)
(2/4) Installing nghttp2-libs (1.39.2-r0)
(3/4) Installing libcurl (7.66.0-r0)
(4/4) Installing curl (7.66.0-r0)
Executing busybox-1.30.1-r2.trigger
Executing ca-certificates-20190108-r0.trigger
OK: 7 MiB in 18 packages
/ # curl bar
Server address: 192.168.234.114:80
Server name: bar-1
Date: 18/Sep/2019:05:55:16 +0000
URI: /
Request ID: e31f3fb14ff1cf8264d684bedc2c0200
/ # curl bar
Server address: 192.168.234.113:80
Server name: bar-0
Date: 18/Sep/2019:05:55:17 +0000
URI: /
Request ID: 97fc549c450ef1360a966eb92af1f8f3
#+END_EXAMPLE

** Эксплуатация

В любой момент времени мы можем отмасштабировать наш Deployment:
#+BEGIN_EXAMPLE
$ kubectl scale statefulset bar --replicas=5
#+END_EXAMPLE

Удаление Deployment:
#+BEGIN_EXAMPLE
$ kubectl delete statefulset bar
#+END_EXAMPLE
